{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Import Library and Dataset"
      ],
      "metadata": {
        "id": "2IgIVgZSZBhI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install scikit-learn matplotlib seaborn\n",
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, SpectralClustering\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "from sklearn.metrics import silhouette_samples\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/UTSClustering.csv', encoding='ISO-8859-1')\n",
        "\n",
        "df.dropna(inplace=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3W0DFDGrxvvE",
        "outputId": "c08ffbcf-a3b5-456f-b9db-efd4578b2612"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Features Selection"
      ],
      "metadata": {
        "id": "rIEJJQCjZOw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove constant features\n",
        "constant_features = [col for col in df.columns if df[col].nunique() == 1]\n",
        "df.drop(columns=constant_features, inplace=True)\n",
        "\n",
        "# Remove highly correlated features\n",
        "numerical_df = df.select_dtypes(include=[np.number])\n",
        "corr_matrix = numerical_df.corr().abs()\n",
        "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "to_drop = [column for column in upper.columns if any(upper[column] > 0.9)]\n",
        "df.drop(columns=to_drop, inplace=True)"
      ],
      "metadata": {
        "id": "GOxzGpPAKk9V"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Feature Engineering Standardization"
      ],
      "metadata": {
        "id": "MdTncGo4ZRrO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select numerical columns only\n",
        "numerical_df = df.select_dtypes(include=[np.number])\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(numerical_df)"
      ],
      "metadata": {
        "id": "OSQpbX42KlDO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Clustering Models"
      ],
      "metadata": {
        "id": "aiZAUn0nZX1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "    'KMeans': KMeans(n_clusters=3, random_state=42),\n",
        "    'Agglomerative Clustering': AgglomerativeClustering(n_clusters=3),\n",
        "    'DBSCAN': DBSCAN(eps=0.5, min_samples=5),\n",
        "    'Gaussian Mixture': GaussianMixture(n_components=3, random_state=42),\n",
        "    'Spectral Clustering': SpectralClustering(n_clusters=3, affinity='nearest_neighbors', random_state=42)\n",
        "}"
      ],
      "metadata": {
        "id": "YXorI64AKlJZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Fit The Models and Evaluate"
      ],
      "metadata": {
        "id": "Wf9sWxBGZaN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    if name == 'DBSCAN':  # DBSCAN does not have a predict method\n",
        "        model.fit(scaled_data)\n",
        "        cluster_labels = model.labels_\n",
        "    else:\n",
        "        cluster_labels = model.fit_predict(scaled_data)\n",
        "\n",
        "    # Some models may label all as noise (-1), skip if all labels same\n",
        "    if len(set(cluster_labels)) > 1:\n",
        "        silhouette = silhouette_score(scaled_data, cluster_labels)\n",
        "        davies_bouldin = davies_bouldin_score(scaled_data, cluster_labels)\n",
        "        calinski_harabasz = calinski_harabasz_score(scaled_data, cluster_labels)\n",
        "    else:\n",
        "        silhouette = davies_bouldin = calinski_harabasz = np.nan\n",
        "\n",
        "    results[name] = {\n",
        "        'Silhouette Score': silhouette,\n",
        "        'Davies-Bouldin Index': davies_bouldin,\n",
        "        'Calinski-Harabasz Score': calinski_harabasz,\n",
        "        'Cluster Labels': cluster_labels\n",
        "    }"
      ],
      "metadata": {
        "id": "AJLceNCoKlO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Visualize Evaluation Metrics"
      ],
      "metadata": {
        "id": "U1ECvPIRatTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_df = pd.DataFrame(results).T\n",
        "print(metrics_df)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "gE4_KeyjKlU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Choose The Best Model"
      ],
      "metadata": {
        "id": "U_jchE_eaxrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model_name = metrics_df['Silhouette Score'].idxmax()\n",
        "best_model = models[best_model_name]\n",
        "best_labels = results[best_model_name]['Cluster Labels']"
      ],
      "metadata": {
        "id": "PhdJQAq_KlaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Visualize Clusters"
      ],
      "metadata": {
        "id": "wpQp0CA6a2X9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=numerical_df.iloc[:, 0], y=numerical_df.iloc[:, 1], hue=best_labels, palette='viridis', s=100)\n",
        "plt.title(f\"Cluster Scatter Plot - {best_model_name}\")\n",
        "plt.xlabel(numerical_df.columns[0])\n",
        "plt.ylabel(numerical_df.columns[1])\n",
        "plt.legend(title='Cluster')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uDMJx0ZUKlfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Silhouette Plot"
      ],
      "metadata": {
        "id": "M7WGlCSEa6Dn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "silhouette_vals = silhouette_samples(scaled_data, best_labels)\n",
        "sns.histplot(silhouette_vals, kde=True, bins=20, color=\"blue\")\n",
        "plt.title(f\"Silhouette Plot - {best_model_name}\")\n",
        "plt.xlabel('Silhouette Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hG7bQ3FwKllT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dendrogram"
      ],
      "metadata": {
        "id": "CB4SUKZga_-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if best_model_name == 'Agglomerative Clustering':\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    linked = linkage(scaled_data, method='ward')\n",
        "    dendrogram(linked)\n",
        "    plt.title('Dendrogram - Agglomerative Clustering')\n",
        "    plt.xlabel('Sample Index')\n",
        "    plt.ylabel('Distance')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "w9rI9q9cKlqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Final Conclusion"
      ],
      "metadata": {
        "id": "e1yv-A7ZbEGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model_description = {\n",
        "    'KMeans': 'KMeans is a centroid-based clustering method that tries to minimize variance within clusters.',\n",
        "    'Agglomerative Clustering': 'Agglomerative Clustering is a hierarchical clustering algorithm that builds a tree of clusters.',\n",
        "    'DBSCAN': 'DBSCAN is a density-based algorithm that groups closely packed points and labels outliers as noise.',\n",
        "    'Gaussian Mixture': 'Gaussian Mixture is a probabilistic model assuming data from several Gaussian distributions.',\n",
        "    'Spectral Clustering': 'Spectral Clustering uses eigenvalues of a similarity matrix to reduce dimensions before clustering.'\n",
        "}\n",
        "\n",
        "print(f\"Best Model: {best_model_name}\")\n",
        "print(best_model_description[best_model_name])"
      ],
      "metadata": {
        "id": "nA0QjhEcKlxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Tidak Dapat dieksekusi karena RAM penuh tetapi RAM masih banyak kosongnya**"
      ],
      "metadata": {
        "id": "-xWDDIzVI-Iq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Penjelasan**\n",
        "**1. Pengumpulan dan Pembersihkan Data**\n",
        "- Menggunakan Pandas untuk membaca data dan membersihkan\n",
        "\n",
        "**2. Pengumpulan dan Pembersihkan Data**\n",
        "- Melakukan seleksi fitur menggunakan teknik yang disebutkan\n",
        "- Melakukan transformasi data seperti standardisasi atau normalisasi untuk meningkatkan performa model\n",
        "\n",
        "**3. Pengumpulan dan Pembersihkan Data**\n",
        "- Menjalankan berbagai Algoritma Clustering, seperti KMeans, AgglomerativeClustering, DBSCAN, GaussianMixture, dan SpectralClustering.\n",
        "\n",
        "**4. Evaluasi**\n",
        "- Menggunakan metrik evaluasi seperti Silhouette Score, Davies-Bouldin Index, dan Calinski-Harabasz Score.\n",
        "\n",
        "**5. Visualisasi**\n",
        "- Visualisasi dengan Silhouette Plot, Dendrogram, dan CLuster Scatter Plot"
      ],
      "metadata": {
        "id": "JELtZEDRzoKZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Analisis**\n",
        "\n",
        "**1. Inkonsistensi antara Elbow Method dan Silhouette Score pada K-Means**\\\n",
        "Penyebab:\n",
        "- Elbow Method mengidentifikasi jumlah cluster (K=5) dengan meminimalkan within-cluster sum of squares (WCSS), namun ini tidak selalu mencerminkan kualitas pemisahan cluster. Elbow method hanya mencari titik perubahan dalam penurunan WCSS tanpa mempertimbangkan bagaimana cluster tersebut terpisah dengan jelas.\n",
        "- Silhouette Score yang rendah (0.3) menunjukkan bahwa meskipun cluster tersebut padat, kemungkinan tumpang tindih atau tidak cukup terpisah dengan baik.\n",
        "\n",
        "Strategi Validasi Alternatif\n",
        "- **Gap Statistic**\\\n",
        "Menghitung gap antara WCSS yang dihasilkan oleh model clustering terhadap distribusi acak untuk menemukan jumlah cluster yang optimal, dengan memperhatikan struktur data yang lebih kompleks.\n",
        "- **Validasi Stabilitas via Bootstrapping**\\\n",
        "Menggunakan bootstrapping untuk menguji kestabilan cluster pada subset acak dari data.\n",
        "- **Distribusi Non-Spherical**\\\n",
        "Jika data memiliki distribusi non-spherical, seperti elips atau klaster yang lebih kompleks, K-Means bisa gagal karena algoritma ini mengasumsikan cluster berbentuk bulat.\n",
        "\n",
        "**2. Preprocessing untuk Fitur Numerik dan Kategorikal High-Cardinality**\n",
        "\n",
        "**Metode Preprocessing untuk Fitur Numerik dan Kategorikal**\n",
        "- **Fitur Numerik (Quantity, UnitPrice)**\n",
        "Standardisasi atau normalisasi penting agar fitur numerik memiliki skala yang seragam, sehingga model tidak terpengaruh oleh perbedaan skala yang besar antara fitur-fitur tersebut.\n",
        "- **Fitur Kategorikal High-Cardinality (Description)**\\\n",
        "Untuk menangani fitur teks dengan nilai kategori yang sangat banyak, metode encoding yang lebih canggih diperlukan.\n",
        "\n",
        "**Risiko One-Hot Encoding**\n",
        "- **High Cardinality**\\\n",
        "One-Hot Encoding mengubah fitur kategorikal menjadi vektor biner dengan dimensi yang sangat tinggi, yang menyebabkan masalah dalam komputasi dan dapat mengarah pada curse of dimensionality.\n",
        "\n",
        "**Mengapa TF-IDF atau UMAP lebih baik?**\n",
        "- TF-IDF dan UMAP lebih cocok karena mereka mengatasi masalah dimensionalitas tinggi dan memberi representasi yang lebih kompak dan bermakna untuk data kategorikal atau teks. Ini juga dapat lebih baik dalam menangkap pola dari data yang sangat terdistribusi.\n",
        "\n",
        "**3. Menentukan Nilai Optimal Epsilon pada DBSCAN**\n",
        "\n",
        "**Peran K-Distance Graph dan Kuartil Ke-3**\n",
        "- K-Distance Graph digunakan untuk mencari nilai epsilon yang tepat untuk DBSCAN.\n",
        "- Kuartil Ke-3 dari distribusi jarak ini bisa digunakan untuk memilih nilai epsilon secara adaptif.\n",
        "\n",
        "**Menyesuaikan MinPts**\n",
        "- MinPts (minimal points per cluster) harus disesuaikan berdasarkan kerapatan regional data. Di area dengan kepadatan tinggi, lebih sedikit titik (MinPts rendah) mungkin sudah cukup untuk membentuk cluster.\n",
        "\n",
        "**4. Memperbaiki Pemisahan Cluster dengan Teknik Semi-Supervised**\n",
        "\n",
        "**Mengatasi Overlap antara CLuster \"High-Value Customers\" dan \"Bulk Buyers\"**\n",
        "- **Constrained Clustering (Semi-Supervised)**\\\n",
        "Teknik ini melibatkan pemberian beberapa batasan atau constraints pada algoritma clustering, seperti memberitahu algoritma bahwa dua titik tertentu harus berada dalam cluster yang sama (must-link) atau berbeda (cannot-link).\n",
        "- **Integrasi Metric Learning (Mahalanobis Distance)**\\\n",
        "Mahalanobis distance lebih robust dalam mengukur jarak antara titik-titik yang berbeda distribusi, dengan mempertimbangkan variansi dan korelasi antar fitur.\n",
        "\n",
        "**Tantangan Interpretabilitas Bisnis**\n",
        "- Pendekatan non-Euclidean seperti Mahalanobis distance dan constrained clustering bisa memperumit interpretasi bisnis, karena jarak antar titik tidak lagi berbasis pada ruang Euclidean tradisional.\n",
        "\n",
        "**5. Desain Fitur Temporal dan Risiko Data Leakage**\n",
        "\n",
        "**Desain Fitur Temporal dari InvoiceDate**\n",
        "- **Hari dalam Minggu dan Jam Pembelian**\\\n",
        "Fitur seperti day of the week atau hour of purchase bisa digunakan untuk mengidentifikasi pola pembelian periodik, seperti pembelian lebih banyak di pagi hari atau malam hari.\n",
        "\n",
        "**Risiko Data Leakage dengan Agregasi Temporal**\n",
        "- **Data Leakage**\\\n",
        "Jika kita menggunakan rata-rata pembelian bulanan tanpa melakukan time-based cross-validation, kita bisa memasukkan informasi masa depan ke dalam model, yang mengarah pada data leakage.\n",
        "- **Lag Features**\\\n",
        "Lag features misalnya, pembelian 7 hari sebelumnya dapat memperkenalkan noise karena mereka mengandalkan data masa lalu yang tidak relevan, atau pola musiman yang tidak konsisten."
      ],
      "metadata": {
        "id": "65qtfW5e2ELN"
      }
    }
  ]
}